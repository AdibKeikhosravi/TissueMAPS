#! /usr/bin/env python
# encoding: utf-8

from os.path import isdir, join, basename
from os import listdir
import numpy as np
import h5py
import yaml
import glob
import re
from copy import copy
from image_toolbox import config
from datafusion import config as df_config
from illuminati.util import ImageSite, Cycles, Project
from image_toolbox.util import load_config

import ipdb as db


'''
Script for fusing measurement data generated by Jterator for use with TissueMAPS.

Jterator generates an HDF5 file for each job. Here we load all files for a
particular experiment (potentially consisting of different sub-experiments,
i.e. cycles) and merge their content into a single HDF5 file.
'''

header_mapper = df_config['CHANNEL_MAPPER']

names = list()
count = 0
for i, cycle in header_mapper.iteritems():
    if isinstance(cycle, dict):
        for old, new in cycle.iteritems():
            count += 1
            names.append(new)

if len(np.unique(names)) < count:
    raise Exception('Names have to be unique.')


def get_project_names(cycle_dir):
    '''
    Get name of a Jterator project, i.e. a sub-folder in the experiment
    directory that contains a .pipe file.
    '''
    return [f for f in listdir(cycle_dir)
            if isdir(join(cycle_dir, f))
            and glob.glob(join(cycle_dir, f, '*.pipe'))]


def image_name_from_joblist(project_dir, data_filename):
    joblist_filename = glob.glob(join(project_dir, '*.jobs'))[0]
    joblist = yaml.load(open(joblist_filename).read())
    job_id = int(re.search(r'_(\d+).data$', data_filename).group(1).lstrip('0'))
    return joblist[job_id].values()[0]


def build_ids(orig_obj_ids, row, col):
    # TODO: generate final, global cell IDs
    return ['%s-%s-%s' % (row, col, int(i)) for i in orig_obj_ids]


def rename_features(feature_names, mapper):
    '''
    Rename feature, i.e. replace substring in a feature name by another
    as defined in a 'mapper' dictionary.

    Parameters:
        :feature_names:     List of strings.
        :mapper:            Dictionary. keys - old names, values - new names.

    Returns:
        renamed features (list of strings)
    '''
    for i, feature in enumerate(feature_names):
        for j, substring in enumerate(mapper.keys()):
            r = re.compile(substring)
            match = re.search(r, feature)
            if match:
                feature_names[i] = re.sub(mapper.keys()[j],
                                          mapper.values()[j],
                                          feature_names[i])
    return feature_names


def get_data_files(project_dir):
    '''
    List all data HDF5 files for a given Jterator project.
    '''
    return sorted(glob.glob(join(project_dir, 'data', '*.data')))


def merge_data(data_files, names, as_int=False):
    '''
    Merge Jterator data of one experiment cycle stored in several HDF5 files.

    Parameters:
        :data_files:        List of strings. Path to Jterator data files.
        :names:             List of strings. Name of a dataset in a HDF5 file.
        :as_int:            Boolean. Convert to integer datatype.

    Returns:
        data (numpy array) of shape nxp,
        where n is the number of objects and p the number of features
    '''
    for job_ix, filename in enumerate(data_files):

        f = h5py.File(filename, 'r')
        if not f.keys():
            raise Exception('File "%s" is empty' % filename)

        for i, name in enumerate(names):

            if name not in f.keys():
                raise Exception('Dataset "%s" does not exist in file "%s'
                                % (name, filename))

            if name == 'OriginalObjectIds':
                # Get positional information from filename
                im_file = image_name_from_joblist(project_dir, filename)
                coords = ImageSite.from_filename(im_file, config)
                # Translate site specific ids to global ids
                nitems = len(f.values()[0])
                feat = build_ids(orig_obj_ids=f['OriginalObjectIds'][:nitems],
                                 row=coords.row_nr, col=coords.col_nr)
            else:
                # it seems that features saved with Matlab have different
                # shapes when compared to those saved with Python???
                feat = f[name][()]
                feat = np.matrix(feat)
                if feat.shape[0] < feat.shape[1]:
                    feat = feat.T
            if i == 0:
                feature = feat
            else:
                feature = np.hstack((feature, feat))

        f.close()

        if job_ix == 0:
            data = feature
        else:
            data = np.vstack((data, feature))

    if as_int:
        return data.astype(int)
    else:
        return data


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='Merge data generated by Jterator for use in tissueMAPS.')

    parser.add_argument('experiment_dir', nargs='*',
                        help='absolute path to experiment directory')

    parser.add_argument('-o', '--output', dest='output_dir', required=True,
                        help='directory where the HDF5 file should be saved')

    parser.add_argument('-c', '--config', dest='config',
                        help='path to custom yaml configuration file \
                        (defaults to "datafusion" configuration)')

    args = parser.parse_args()

    if args.config:
        # Overwrite default "datafusion" configuration
        df_config = load_config(args.config)

    exp_dir = args.experiment_dir[0]
    output_dir = args.output_dir

    if not exp_dir:
        raise Exception('Experiment directory "%s" does not exist.'
                        % exp_dir)

    if not output_dir:
        raise Exception('Output directory "%s" does not exist.' % output_dir)

    output_filename = join(output_dir, 'data.h5')

    cycles = Cycles(config)
    cycles = cycles.get_cycle_directories(exp_dir)

    data_header = list()
    for i, cycle in enumerate(cycles):

        print('. Extracting features of cycle #%d: %s' %
              (cycle.cycle_number, cycle.cycle_name))

        cycle_dir = join(exp_dir, cycle.cycle_name)
        project_names = get_project_names(cycle_dir)

        for project_name in project_names:

            project_dir = join(cycle_dir, project_name)

            if project_dir == df_config['SEGMENTATION_PROJECT'].format(cycle_subdirectory=cycle_dir):
                # data of segmentation project is handles separately
                continue

            print '.. Merging data of project "%s"' % project_name

            data_files = get_data_files(project_dir)

            f = h5py.File(data_files[0], 'r')
            feature_names = f.keys()
            f.close()

            if 'OriginalObjectIds' not in feature_names:
                raise Exception('Files must contain a dataset called "%s"' %
                                'OriginalObjectIds')

            features = merge_data(data_files, names=feature_names)

            if features.shape[1] != len(feature_names):
                raise Exception('Number of features in dataset and length of '
                                'list with feature names must be identical.')

            if i == 0:
                data = features
            else:
                data = np.vstack((data, features))

            feature_names = rename_features(feature_names,
                                            header_mapper[cycle.cycle_number])
            # convert to safe string format for HDF5
            feature_names = np.array(map(np.string_, feature_names))
            data_header += feature_names

    print '. Combining data from different cycles'
    (data_header, unique_ix) = np.unique(data_header, return_index=True)
    # TODO: sanity checks
    data = data[:, unique_ix]

    ids_ix = np.where([feat == 'OriginalObjectIds' for feat in feature_names])
    ids = data[:, ids_ix]
    np.delete(data, ids_ix, axis=1)

    print '. Separate features belonging to different objects'
    objects = [re.match(r'^([^_]+)', name).group(1) for name in data_header]
    (objects, object_ix) = np.unique(objects, return_inverse=True)

    print '. Writing fused data into HDF5 file "%s"' % output_filename
    f = h5py.File(output_filename, 'w')  # truncate file if exists
    f.create_dataset('parent', data=np.string_('cells'))  # hard-coded for now
    for i, obj in enumerate(objects):

        print '. Writing data for object "%s"' % obj

        obj_ix = object_ix == i
        obj_name = obj.lower()  # use lower case consistently

        ids = merge_data(project_dir, names=['OriginalObjectIds'],
                         as_int=True)
        f.create_dataset('/%s/ids' % obj_name, data=ids)

        data_files = get_data_files(df_config['SEGMENTATION_PROJECT'])

        centroids = merge_data(data_files, names=['%s_Centroids' % obj],
                               as_int=True)
        location = '/%s/centroids' % obj_name
        f.create_dataset(location, data=centroids)
        f[location].attrs.__setitem__('names', np.array(['y', 'x']))

        boundaries = merge_data(data_files, names=['%s_Boundary' % obj],
                                as_int=True)
        location = '/%s/boundaries' % obj_name
        f.create_dataset(location, data=boundaries)
        f[location].attrs.__setitem__('names', np.array(['y', 'x']))

        border = merge_data(data_files, names=['%s_BorderIx' % obj],
                            as_int=True)
        f.create_dataset('/%s/border' % obj_name, data=border)

        location = '/%s/features' % obj_name
        f.create_dataset(location, data=features[:, obj_ix])
        # Add the 'data_header' as an attribute
        f[location].attrs.__setitem__('names', data_header[obj_ix])

    f.close()
