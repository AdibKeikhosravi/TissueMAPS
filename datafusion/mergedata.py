#! /usr/bin/env python
# encoding: utf-8

from os.path import isdir, join, basename
from os import listdir
import numpy as np
import h5py
import yaml
import glob
import re
from copy import copy
from image_toolbox import config
from datafusion import config as df_config
from illuminati.util import ImageSite, Cycles, Project
from image_toolbox.util import load_config

import ipdb as db


'''
Script for fusing measurement data generated by Jterator for use with TissueMAPS.

Jterator generates an HDF5 file for each job. Here we load all files for a
particular experiment (potentially consisting of different sub-experiments,
i.e. cycles) and merge their content into a single HDF5 file.
'''

header_mapper = df_config['CHANNEL_MAPPER']

names = list()
count = 0
for i, cycle in header_mapper.iteritems():
    if isinstance(cycle, dict):
        for old, new in cycle.iteritems():
            count += 1
            names.append(new)

if len(np.unique(names)) < count:
    raise Exception('Names have to be unique.')


def get_project_names(cycle_dir):
    '''
    Get name of a Jterator project, i.e. a sub-folder in the experiment
    directory that contains a .pipe file.
    '''
    return [f for f in listdir(cycle_dir)
            if isdir(join(cycle_dir, f))
            and glob.glob(join(cycle_dir, f, '*.pipe'))]


def image_name_from_joblist(project_dir, data_filename):
    joblist_filename = glob.glob(join(project_dir, '*.jobs'))[0]
    joblist = yaml.load(open(joblist_filename).read())
    job_id = int(re.search(r'_(\d+).data$', data_filename).group(1).lstrip('0'))
    return joblist[job_id].values()[0]


def build_ids(orig_obj_ids, row, col):
    # TODO: generate final, global cell IDs
    return ['%s-%s-%s' % (row, col, int(i)) for i in orig_obj_ids]


def rename_features(feature_names, mapper):
    '''
    Rename feature, i.e. replace substring in a feature name by another
    as defined in a 'mapper' dictionary.

    Parameters:
        :feature_names:     List of strings.
        :mapper:            Dictionary. keys - old names, values - new names.

    Returns:
        renamed features (list of strings)
    '''
    for i, feature in enumerate(feature_names):
        for j, substring in enumerate(mapper.keys()):
            r = re.compile(substring)
            match = re.search(r, feature)
            if match:
                feature_names[i] = re.sub(mapper.keys()[j],
                                          mapper.values()[j],
                                          feature_names[i])
    return feature_names


def check1(features):
    check1 = map(len, features.values())
    if len(np.unique(check1)) == 1:
        print('🍺  Check 1 passed: '
              'all sites have same number of features')
    else:
        raise Exception('Sites have a different number of features.')


def check2(features):
    check2 = [feat.shape[0] for feat in features]
    filter(lambda i: i[1] != check2[0], enumerate(check2))  # ignore empty sites
    if len(np.unique(check2)) == 1:
        print('🍺  Check 2 passed: '
              'all features have same number of sub-features')
    else:
        raise Exception('Features have a different number of sub-features.')


def merge_data(project_dir):
    '''
    Merge Jterator data of one experiment cycle stored in several HDF5 files.

    Parameters:
        :project_dir:       String. Path to Jterator project folder.

    Returns:
        tuple with features per object (numpy array),
        feature names (list of strings) and object ids (list of integers)
    '''
    # TODO: this is certainly not the best way to do it!!!
    data_files = glob.glob(join(project_dir, 'data', '*.data'))

    f = h5py.File(data_files[0], 'r')
    feature_names = f.keys()
    f.close()
    feature_names = [feat for feat in feature_names
                     if not re.search('OriginalObjectIds', feat)]
    features = {feat: [] for feat in feature_names}

    ids = list()
    for job_ix, filename in enumerate(data_files):

        f = h5py.File(filename, 'r')
        if not f.keys():
            raise Exception('File "%s" is empty' % filename)

        for n in feature_names:
            if n in f.keys():
                features[n].append(np.matrix(f[n][()]).T)  # use matrix!!!
            else:
                print('Warning: dataset "%s" does not exist in file "%s'
                      % (n, filename))
                features[n].append(None)

        if 'OriginalObjectIds' not in f.keys():
            raise Exception('File "%s" must contain a data called "%s"'
                            % (filename, 'OriginalObjectIds'))
        # Get positional information from filename
        image_filename = image_name_from_joblist(project_dir, filename)
        coords = ImageSite.from_filename(image_filename, config)
        # Translate site specific ids to global ids
        nitems = len(f.values()[0])
        ids += build_ids(orig_obj_ids=f['OriginalObjectIds'][:nitems],
                         row=coords.row_nr, col=coords.col_nr)

        f.close()

    check1(features)

    # Combine features per site into a vector
    features = np.array([np.vstack(feat) for feat in features.values()])

    check2(features)

    # Combine features into a nxp numpy array,
    # where n is the number of cells and p is the number of features
    features = np.hstack(features)
    feature_names = rename_features(feature_names,
                                    header_mapper[cycle.cycle_number])
    feature_names = np.array(map(np.string_, feature_names))  # safest for hdf5

    return (features, feature_names, ids)


def merge_metadata(project_dir, name):
    '''
    Merge Jterator metadata (segmentation data) of one experiment cycle
    stored in several HDF5 files.

    Parameters:
        :project_dir:       String. Path to Jterator project folder.
        :name:              String. Name of a dataset in a HDF5 file.

    Returns:
        feature (numpy array)
    '''
    data_files = glob.glob(join(project_dir, 'data', '*.data'))

    for job_ix, filename in enumerate(data_files):

        f = h5py.File(filename, 'r')
        if not f.keys():
            raise Exception('File "%s" is empty' % filename)

        if name in f.keys():
            feat = f[name][()].astype(int)
            if job_ix == 0:
                feature = feat
            else:
                feature = np.vstack((feature, feat))
        else:
            raise Exception('Warning: dataset "%s" does not exist in file "%s'
                            % (name, filename))

        f.close()

    return feature


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='Merge data generated by Jterator for use in tissueMAPS.')

    parser.add_argument('experiment_dir', nargs='*',
                        help='absolute path to experiment directory')

    parser.add_argument('-o', '--output', dest='output_dir', required=True,
                        help='directory where the HDF5 file should be saved')

    parser.add_argument('-c', '--config', dest='config',
                        help='path to custom yaml configuration file \
                        (defaults to "datafusion" configuration)')

    args = parser.parse_args()

    if args.config:
        # Overwrite default "datafusion" configuration
        df_config = load_config(args.config)

    exp_dir = args.experiment_dir[0]
    output_dir = args.output_dir

    if not exp_dir:
        raise Exception('Experiment directory "%s" does not exist.' % exp_dir)

    if not output_dir:
        raise Exception('Output directory "%s" does not exist.' % output_dir)

    output_filename = join(output_dir, 'data.h5')

    cycles = Cycles(config)
    cycles = cycles.get_cycle_directories(exp_dir)

    data_header = list()
    metadata_header = list()
    for i, cycle in enumerate(cycles):

        print('. Extracting features of cycle #%d: %s'
              % (cycle.cycle_number, cycle.cycle_name))

        cycle_dir = join(exp_dir, cycle.cycle_name)
        project_names = get_project_names(cycle_dir)

        for project_name in project_names:

            project_dir = join(cycle_dir, project_name)

            print '.. Merging data of project "%s"' % project_name

            if project_dir == df_config['SEGMENTATION_PROJECT']:
                continue
            else:
                (features, feature_names, obj_ids) = merge_data(project_dir)
                if i == 0:
                    data = features
                else:
                    data = np.hstack((data, features))
                data_header += feature_names

    print '. Combining data from different cycles'
    (data_header, unique_ix) = np.unique(data_header, return_index=True)
    # TODO: sanity checks
    data = data[:, unique_ix]

    print '. Separate features belonging to different objects'
    objects = [re.match(r'^([^_]+)', name).group(1) for name in data_header]
    (objects, object_ix) = np.unique(objects, return_inverse=True)

    print '. Writing fused data into HDF5 file "%s"' % output_filename
    f = h5py.File(output_filename, 'w')  # truncate file if exists
    f.create_dataset('parent', data=np.string_('cells'))  # hard-coded for now
    for i, obj in enumerate(objects):

        print '. Writing data for object "%s"' % obj

        obj_ix = object_ix == i
        obj_name = obj.lower()  # use lower case consistently

        f.create_dataset('/%s/ids' % obj_name, data=np.array(obj_ids))

        centroids = merge_metadata(df_config['SEGMENTATION_PROJECT'],
                                   name='%s_Centroids' % obj)
        location = '/%s/centroids' % obj_name
        f.create_dataset(location, data=centroids)
        f[location].attrs.__setitem__('names', np.array(['y', 'x']))

        boundaries = merge_metadata(df_config['SEGMENTATION_PROJECT'],
                                    name='%s_Boundary' % obj)
        location = '/%s/boundaries' % obj_name
        f.create_dataset(location, data=boundaries)
        f[location].attrs.__setitem__('names', np.array(['y', 'x']))

        border = merge_metadata(df_config['SEGMENTATION_PROJECT'],
                                name='%s_BorderIx' % obj)
        f.create_dataset('/%s/border' % obj_name, data=border)

        location = '/%s/features' % obj_name
        f.create_dataset(location, data=data[:, obj_ix])
        # Add the 'data_header' as an attribute
        f[location].attrs.__setitem__('names', data_header[obj_ix])

    f.close()
