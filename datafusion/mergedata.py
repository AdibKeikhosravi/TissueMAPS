#! /usr/bin/env python
# encoding: utf-8

import os
import numpy as np
import h5py
import glob
import re
from image_toolbox import config
from datafusion import df_config
from illuminati.util import ImageSite


'''
Script for fusing measurement data generated by Jterator for use with TissueMAPS.

Jterator generates an HDF5 file for each job. Here we load all files for a
particular experiment (potentially consisting of different sub-experiments,
i.e. cycles) and merge their content into a single HDF5 file.
'''

# TODO: yaml config file
# header_mapper = {
#     'Dapi': 'DAPI',
#     'Red': 'EEA1',
#     'Celltrace': 'SE'
# }

header_mapper = df_config

cycle_mapper = {
    '_01': 'cycle1',
    '_02': 'cycle11',
    '_03': 'cycle12'
}


def build_ids(original_object_ids, row_id, col_id):
    # TODO: generate final, global cell IDs
    ids = [
        '{row}-{col}-{local}'.format(row=row_id, col=col_id, local=local_id)
            for local_id in original_object_ids
    ]
    return ids


def merge_data(project_dir, output_dir, rename):

    project_name = os.path.basename(project_dir)
    # output_filename = os.path.join(output_dir, '%s.features' % project_name)
    output_filename = os.path.join(output_dir, 'features.h5')

    cycle_dirs = glob.glob(os.path.join(project_dir, '%s*' % project_name))

    # Loop over cycles

    first_cycle = True
    for cycle in cycle_dirs:

        print('. Processing cycle # %d' %
              int(re.search('%s.?(\d+)$' % project_name, cycle).group(1)))

        data_dir = os.path.join(cycle, 'data')
        data_files = glob.glob(os.path.join(data_dir, '*.data'))

        f = h5py.File(data_files[0], 'r')
        groups = f.keys()
        f.close()
        groups.remove('OriginalObjectIds')
        features = {k: [] for k in groups}

        print '.. Reading data from files'

        ids = []
        for filename in data_files:

            f = h5py.File(filename, 'r')
            if not f.keys():
                print 'Warning: file "%s" is emtpy' % filename
                continue

            # Convert site specific ids to global ids for tissueMAPS
            # Get positional information from filename
            site = int(re.search(r'(\d{5})\.data', filename).group(1))
            image_folder = os.path.join(data_dir, '../TIFF')
            (row, col) = SiteImage.from_filename(filename, config)
            # (row, col) = get_pos_from_filename(site, image_folder)
            # Build identifier string
            nitems = len(f.values()[0])
            ids += build_ids(f['OriginalObjectIds'][:nitems], row, col)

            # Read measurement data
            for g in groups:
                if g in f.keys():
                    features[g].append(np.matrix(f[g][()]).T)  # use matrix!!!
                else:
                    print('Warning: group "%s" does not exist in file "%s' %
                          (g, filename))
                    features[g].append(None)
            f.close()

        # Check #1: do all sites (jobs) have the same number of features?
        check1 = map(len, features.values())

        if len(np.unique(check1)) == 1:
            print 'üç∫  Check 1 passed'
        else:
            raise Exception('Sites have a different number of features.')

        # Combine features per site into a vector
        dataset = np.array([np.vstack(feat) for feat in features.values()])

        # Check #2: do all features have the same number of measurements?
        check2 = [feat.shape[0] for feat in dataset]
        # ignore empty sites
        filter(lambda i: i[1] != check2[0], enumerate(check2))

        if len(np.unique(check2)) == 1:
            print 'üç∫  Check 2 passed'
        else:
            raise Exception('Features have a different number of measurements.')

        # Combine features into one nxp numpy array,
        # where n is the number of single-cell measurements
        # and p is the number of features
        dataset = np.hstack(dataset)
        header = groups
        dataset_name = os.path.basename(cycle)

        # Rename features for better interpretation
        if rename:
            print '.. Renaming features'
            for i, feat in enumerate(header):
                for j, substring in enumerate(header_mapper.keys()):
                    r = re.compile(substring)
                    match = re.search(r, feat)
                    if match:
                        header[i] = re.sub(header_mapper.keys()[j],
                                           header_mapper.values()[j],
                                           header[i])

            print '.. Renaming cycles'
            for j, substring in enumerate(cycle_mapper.keys()):
                r = re.compile(substring)
                match = re.search(r, dataset_name)
                if match:
                    dataset_name = cycle_mapper.values()[j]

        header = np.array(map(np.string_, groups))  # safest for hdf5

        print '.. Writing merged data into HDF5 file'

        if first_cycle:
            f = h5py.File(output_filename, 'w')  # truncate file if exists
            # Ids are the same for each cycle, so we store it in the root group
            f.create_dataset('/ids', data=np.array(ids))
            f.close()

        # Write merged data into a new hdf5 file
        f = h5py.File(output_filename, 'a')
        location = '/%s' % dataset_name
        # Write the dataset
        f.create_dataset(location, data=dataset)
        # Add the 'header' metadata as an attribute
        f[location].attrs.__setitem__('header', header)

        f.close()
        first_cycle = False


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='Merge data generated by Jterator for use in tissueMAPS.')

    parser.add_argument('project_dir', nargs='*',
                        help='absolute path to project directory')

    parser.add_argument('-o', '--output', dest='output_dir', required=True,
                        help='directory where the HDF5 file should be saved')

    parser.add_argument('-r', '--rename', dest='rename',
                        default=False, action='store_true',
                        help='rename features according to \'header_mapper\'')

    args = parser.parse_args()

    project_dir = args.project_dir[0]
    output_dir = args.output_dir

    if not project_dir:
        raise Exception('Project directory "%s" does not exist.' % project_dir)

    if not output_dir:
        raise Exception('Output directory "%s" does not exist.' % output_dir)

    merge_data(project_dir, output_dir, args.rename)
