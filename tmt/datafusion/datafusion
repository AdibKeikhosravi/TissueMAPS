#! /usr/bin/env python
# encoding: utf-8

from os.path import isdir, join, basename, dirname
from os import listdir
import numpy as np
import pandas as pd
import h5py
import yaml
import glob
import re
import tmt
from tmt import config
import natsort
import tmt.datafusion
from tmt.image import Image
from tmt.experiment import Experiment
from tmt.util import load_config


# TODO: use Jtproject class and modify it in a way that it can be used
# by JtUI and datafusion


header_mapper = tmt.datafusion.df_config['CHANNEL_MAPPER']

names = list()
count = 0
for i, cycle in header_mapper.iteritems():
    if isinstance(cycle, dict):
        for old, new in cycle.iteritems():
            count += 1
            names.append(new)

if len(np.unique(names)) < count:
    raise Exception('Names have to be unique.')


def list_jtproject_names(cycle_dir):
    '''
    Returns
    -------
    List[str]
    names of Jterator projects
    '''
    return [f for f in listdir(cycle_dir)
            if isdir(join(cycle_dir, f))
            and glob.glob(join(cycle_dir, f, '*.pipe'))]


def image_name_from_joblist(joblist, data_filename):
    '''
    Determine the name of an image file from a YAML joblist description.

    Parameters
    ----------
    joblist: List[Dict[str, str]]
             joblist description for each job
    data_filename: str
                   name of a data HDF5 file that encodes the job id

    Returns
    -------
    str
    filename
    '''
    job_id = int(re.search(r'_(\d+).data$', data_filename).group(1).lstrip('0'))
    return joblist[(job_id-1)].values()[0]  # convert to zero-based indexing!


def rename_features(feature_names, mapper):
    '''
    Rename features, i.e. replace substring in a feature name
    as defined in "mapper".

    Parameters
    ----------
    feature_names: List[str]
    mapper: Dict[str, str]
            key-value pairs -> substring to replace: new substring

    Returns
    -------
    List[str]
    renamed features
    '''
    for i, feature in enumerate(feature_names):
        for j, substring in enumerate(mapper.keys()):
            r = re.compile(substring)
            match = re.search(r, feature)
            if match:
                feature_names[i] = re.sub(mapper.keys()[j],
                                          mapper.values()[j],
                                          feature_names[i])
    return feature_names


def list_data_files(project_dir):
    '''
    Provide a sorted list of data HDF5 files of a given Jterator project.

    Parameters
    ----------
    project_dir: str
                 path to a Jterator project directory

    Returns
    -------
    List[str]
    '''
    return natsort.natsorted(glob.glob(join(project_dir, 'data', '*.data')))


def fuse_data(data_files, names, joblist):
    '''
    Fuse Jterator data of one project stored across several HDF5 files.

    Parameters
    ----------
    data_files: List[str]
                paths to Jterator data files
    names: List[str]
           names of the datasets in the HDF5 files
    joblist: List[Dict[str, str]]
             names of the image files that were used for the analysis

    Returns
    -------
    pandas DataFrame
    fused dataset with dimensions nxp,
    where n is the number of objects and p the number of features
    '''
    features = pd.DataFrame()
    for job_ix, filename in enumerate(data_files):

        f = h5py.File(filename, 'r')
        if not f.keys():
            raise Exception('File "%s" is empty' % filename)

        feature = pd.DataFrame()
        for feat_ix, name in enumerate(names):

            if name not in f.keys():
                raise ValueError('Dataset "%s" does not exist in file "%s'
                                 % (name, filename))

            ids = re.search(r'^([^_])_[^_]*ObjectIds', name, re.IGNORECASE)
            if ids:
                obj_name = ids.group(1)
                # Get positional information from filename
                im_file = image_name_from_joblist(joblist, filename)
                image = Image(im_file, config)
                (row, column) = image.coordinates  # returns 0-based indices
                site = image.site
                # Store information about the site-specific information
                nitems = len(f.values()[0])
                feature['%s_ID_site' % obj_name] = np.repeat(site, nitems)
                feature['%s_ID_row' % obj_name] = np.repeat(row, nitems)
                feature['%s_ID_column' % obj_name] = np.repeat(column, nitems)
                feature['%s_ID_object' % obj_name] = f[ids.group(0)][:nitems]
            else:
                feat = f[name][()]
                # Some features may have more than one column
                if len(feat.shape) > 1:
                    if re.search(r'Centroid', name, re.IGNORECASE):
                        feature['%s_y' % name] = feat[:, 0]
                        feature['%s_x' % name] = feat[:, 1]
                    else:
                        for i in range(feat.shape[1]):
                            feature['%s_%d' % (name, i)] = feat[:, i]
                else:
                    feature[name] = feat

        f.close()

        if job_ix == 0:
            features = feature
        else:
            features = pd.concat([features, feature])

    return features


def build_global_ids(ids):
    '''
    Build global, continuous ids from local, image site specific ids.

    Parameters
    ----------
    ids: pandas DataFrame
         dataset of size nx4 with the following columns:
         "{object name}_ID_site"
         "{object name}_ID_row"
         "{object name}_ID_column"
         "{object name}_ID_object"

    Returns
    -------
    pandas Series
    '''
    n_row = int(np.max(ids.filter(regex='^[^_]+_ID_row')))
    n_col = int(np.max(ids.filter(regex='^[^_]+_ID_column')))
    obj_global_ids = ids['ID_object'].copy()

    offset = 0
    for r in range(1, n_row+1):
        for c in range(1, n_col+1):
            ix = (ids.ID_row == r) & (ids.ID_column == c)
            local_ids = ids['ID_object'][ix]
            obj_global_ids[ix] = local_ids + offset
            offset = np.max(local_ids + offset)

    return obj_global_ids


def calc_global_centroids(local_centroids, ids, image_size):
    '''
    Calculate global centroids from local, image site specific coordinates.

    Parameters
    ----------
    local_centroids: pandas DataFrame
                     size nx2, where n is the number of
                     objects, and the columns are the y, x coordinates
                     of each object at the local site (i.e. in the image)
    ids: pandas DataFrame
         dataset of size nx4 with the following columns:
         "{object name}_ID_site"
         "{object name}_ID_row"
         "{object name}_ID_column"
         "{object name}_ID_object"
    image_size: Tuple[int]
                dimensions of an individual image, i.e. site

    Returns
    -------
    pandas DataFrame
    dataset with dimensions nx2 and columns "y" and "x"
    '''
    n_row = int(np.max(ids.filter(regex='^[^_]+_ID_row')))
    n_col = int(np.max(ids.filter(regex='^[^_]+_ID_column')))
    global_centroids = local_centroids.copy()  # new creates indexing problems?
    global_centroids.columns = ['y', 'x']

    y_offset = 0
    x_offset = 0
    for r in range(1, n_row+1):
        for c in range(1, n_col+1):
            ix = (ids.ID_row == r) & (ids.ID_column == c)
            global_centroids['y'][ix] = local_centroids[0][ix] + y_offset
            global_centroids['x'][ix] = local_centroids[1][ix] + x_offset
            y_offset = np.max(image_size[0] + y_offset)
            x_offset = np.max(image_size[1] + x_offset)

    return global_centroids


if __name__ == '__main__':
    import argparse

    print tmt.datafusion.logo % {'version': tmt.datafusion.__version__}

    descr = '''
    Fuse data generated by Jterator for use in TissueMAPS.
    Jterator generates an HDF5 file for each job. Load all datasets contained
    in the files of a particular experiment (which may be distributed across
    several subexperiments and/or multiple Jterator projects) and fuse and
    store them in a single `data.h5` HDF5 file.
    '''

    parser = argparse.ArgumentParser(description=descr)

    parser.add_argument('experiment_dir', nargs='*',
                        help='absolute path to experiment directory')

    parser.add_argument('-o', '--output', dest='output_dir',
                        help='directory where the HDF5 file should be saved')

    parser.add_argument('-c', '--config', dest='config',
                        help='path to custom yaml configuration file \
                        (defaults to "datafusion" configuration)')

    args = parser.parse_args()

    if args.config:
        # Overwrite default "datafusion" configuration
        df_config = load_config(args.config)
    else:
        df_config = tmt.datafusion.config

    exp_dir = args.experiment_dir[0]
    output_dir = args.output_dir

    if not exp_dir:
        raise ValueError('Project directory "%s" does not exist.' % exp_dir)

    experiment = Experiment(exp_dir, config)
    cycles = experiment.subexperiments

    if args.output_dir:
        if not args.output_dir:
            raise ValueError('Output directory "%s" does not exist.'
                             % args.output_dir)
        output_filename = join(args.output_dir, 'data.h5')
    else:
        output_filename = experiment.data_filename

    # Determine the Jterator project that was used to segment objects.
    # There should be only one and it will be treated separately.
    segmentation_project_dir = df_config['SEGMENTATION_PROJECT'].format(
                                            experiment_dir=exp_dir,
                                            experiment=basename(exp_dir))

    ###########################################################################
    #                 load individual datasets and fuse them                  #
    ###########################################################################

    # TODO: Are empty sites handles consistently?

    data_header = np.array(list(), dtype=np.string_)
    for i, c in enumerate(cycles):

        print('. Extracting features of cycle #%d: %s' %
              (c.cycle, c.name))

        cycle_dir = join(exp_dir, c.name)
        project_names = list_jtproject_names(cycle_dir)

        for project_name in project_names:

            project_dir = join(cycle_dir, project_name)

            print '.. Merging data of Jterator project "%s"' % project_name

            data_files = list_data_files(project_dir)

            f = h5py.File(data_files[0], 'r')
            feature_names = f.keys()
            f.close()

            # We need the joblist, because we may need to obtain
            # positional information from the image filename,
            # which was used to generate the data
            joblist_files = glob.glob(join(dirname(data_files[0]),
                                           '..', '*jobs'))
            # Why does this sometimes break??? `list index out of range`
            with open(joblist_files[0]) as f:
                joblist = yaml.load(f.read())

            if not any([re.search(r'_OriginalObjectIds$', name)
                        for name in feature_names]):
                raise ValueError('Files must contain a dataset containing '
                                 '"OriginalObjectIds"')

            if project_dir == segmentation_project_dir:
                segm_ix = i
                segmentation_data = fuse_data(data_files,
                                              names=feature_names,
                                              joblist=joblist)
                continue

            features = fuse_data(data_files,
                                 names=feature_names,
                                 joblist=joblist)

            if i == 0:
                data = features
            else:
                data = pd.concat([data, features], axis=1)

    print '. Combining data from different cycles'
    data = data.T.groupby(level=0).first().T  # remove duplicate columns

    print '. Separate features belonging to different objects'
    objects = [re.match(r'^([^_]+)', name).group(1) for name in data.columns]
    (objects, object_ix) = np.unique(objects, return_inverse=True)

    ###########################################################################
    #                          write fused dataset                            #
    ###########################################################################

    print '. Writing fused data into HDF5 file "%s"' % output_filename
    f = h5py.File(output_filename, 'w')  # truncate file if exists
    for i, obj in enumerate(objects):

        print '. Writing data for objects "%s"' % obj

        obj_name = obj.lower()  # use lower case consistently

        # Store the actual features
        obj_data = data.filter(regex='^%s_(?!ID_)' % obj)
        location = 'objects/%s/features' % obj_name
        f.create_dataset(location, data=obj_data)
        f[location].attrs.__setitem__('names',
                                      np.array(obj_data.columns.values,
                                               dtype=np.string_))

        # NOTE: Ids and border indices may be produced by both measurement and
        # segmentation projects, depending on whether alignment had to be done
        # in measurement projects. Data of measurement projects have priority
        # over those of segmentation projects!

        import ipdb; ipdb.set_trace()

        id_pattern = '^%s_ID_' % obj
        ids = data.filter(regex=id_pattern)
        if not ids:
            ids = segmentation_data.filter(regex=id_pattern)

        # Build global object ids from local site-specific ids
        obj_global_ids = build_global_ids(ids)

        # Store global object ids
        location = 'objects/%s/ids' % obj_name
        f.create_dataset(location, data=obj_global_ids.applymap(int))

        # Also store local "original" object ids for use outside of TissueMAPS
        location = 'objects/%s/original-ids' % obj_name
        f.create_dataset(location, data=ids.applymap(int))
        f[location].attrs.__setitem__('names',
                                      np.array(ids.columns.values,
                                               dtype=np.string_))


        # Determine parent-child relationship between objects
        try:
            parent_ids = segmentation_data.filter(regex='^%s_ParentId' % obj)
            location = 'objects/%s/original-parent-ids' % obj_name
            f.create_dataset(location, data=parent_ids[ix].applymap(int))
            # TODO: map to global ids
        except ValueError as e:
            # If the current object has no parent ids, it is the parent
            f.create_dataset('parent', data=np.string_(obj_name))

        # Calculate global centroids
        # For this we also need the dimensions of the images.
        # Since they all have the same size,
        # it's sufficient to obtain them from a single image.
        centroids = segmentation_data.filter(regex='^%s_Centroids' % obj)
        image_size = cycles[segm_ix].project.segmentation_files[0].dimensions
        global_centroids = calc_global_centroids(centroids[ix],
                                                 ids, image_size)

        # Store global centroids
        location = 'objects/%s/centroids' % obj_name
        f.create_dataset(location, data=global_centroids.applymap(int))
        f[location].attrs.__setitem__('names',
                                      np.array(global_centroids.columns.values,
                                               dtype=np.string_))

        # Store index for objects at the border of images
        border_pattern = '^%s_BorderIx' % obj
        location = 'objects/%s/border' % obj_name
        border = data.filter(regex=border_pattern)
        if not border:
            border = segmentation_data.filter(regex=border_pattern)
        f.create_dataset(location, data=border.applymap(int))

    f.close()
