#! /usr/bin/env python
# encoding: utf-8

import os
from os.path import join
import numpy as np
import h5py
from glob import glob
import re


'''
Script for merging measurement data generated by "jterator"
for use with "tissueMAPS".

Some things are currently hard-coded and should become more flexible.
A lot of logic (e.g. config file) could be adopted from "illuminati".
'''


def get_pos_from_filename(site, image_folder,
                          site_pattern='*_s%.4d_*',
                          pos_regexp='_r(\d+)_c(\d+)_',
                          one_based=True):
    filename = glob(join(image_folder, site_pattern % site))
    filename = filename[0]

    m = re.search(pos_regexp, filename)
    if not m:
            raise Exception('Can\'t create SiteImage object '
                            'from filename ' + filename)
    else:
        row, col = map(int, m.groups())
        if one_based:
            row -= 1
            col -= 1
        return (row, col)


def build_ids(original_object_ids, row_id, col_id):
    ids = [
            '%d-%d-%d' % (row_id, col_id, local_id)
            for local_id in original_object_ids
    ]
    return ids


def merge_data(project_dir, output_dir, siteinfo):

    project_name = os.path.basename(project_dir)
    output_filename = join(output_dir, '%s.features' % project_name)
    cycle_dirs = glob(join(project_dir, '%s*' % project_name))

    # Loop over cycles

    first_cycle = True
    for cycle in cycle_dirs:

        print('. Processing cycle %d' %
              int(re.search('%s.?(\d+)$' % project_name, cycle).group(1)))

        data_dir = join(cycle, 'data')
        data_files = glob(join(data_dir, '*.data'))

        f = h5py.File(data_files[0], 'r')
        groups = f.keys()
        f.close()
        groups.remove('OriginalObjectIds')
        features = {k: [] for k in groups}

        print '.. Reading data from files'

        ids = []
        for filename in data_files:

            f = h5py.File(filename, 'r')
            if not f.keys():
                print 'Warning: file "%s" is emtpy' % filename
                continue

            # Convert site specific ids to global ids for tissueMAPS
            # Get positional information from filename
            site = int(re.search(r'(\d{5})\.data', filename).group(1))
            siteinfo_row = siteinfo[siteinfo[:, 0] == site][0]

            max_id_up_to_now = siteinfo_row[3]
            local_ids = f['OriginalObjectIds'][()].astype('uint32')
            global_ids = local_ids + max_id_up_to_now
            ids += global_ids.tolist()

            # Read measurement data
            for g in groups:
                if g in f.keys():
                    features[g].append(np.matrix(f[g][()]).T)  # use matrix!!!
                else:
                    print('Warning: group "%s" does not exist in file "%s' %
                          (g, filename))
                    features[g].append(None)
            f.close()

        # Check #1: do all sites (jobs) have the same number of features?
        check1 = map(len, features.values())

        if len(np.unique(check1)) == 1:
            print 'üç∫  Check 1 passed'
        else:
            raise Exception('Sites have a different number of features.')

        # Combine features per site into a vector
        dataset = np.array([np.vstack(feat) for feat in features.values()])

        # Check #2: do all features have the same number of measurements?
        check2 = [feat.shape[0] for feat in dataset]
        # ignore empty sites
        filter(lambda i: i[1] != check2[0], enumerate(check2))

        if len(np.unique(check2)) == 1:
            print 'üç∫  Check 2 passed'
        else:
            raise Exception('Features have a different number of measurements.')

        # Combine features into one nxp numpy array,
        # where n is the number of single-cell measurements
        # and p is the number of features
        dataset = np.hstack(dataset)
        header = np.array(map(np.string_, groups))  # safest for hdf5

        print '.. Writing merged data into HDF5 file'

        # Write merged data into a new hdf5 file
        f = h5py.File(output_filename, 'a')
        location = '%s/features' % os.path.basename(cycle)
        # Write the dataset
        f.create_dataset(location, data=dataset)
        # Add the 'header' metadata as an attribute
        f[location].attrs.__setitem__('names', header)

        if first_cycle:
            # Ids are the same for each cycle, so we store it in the root group
            f.create_dataset('/ids', data=np.array(ids, dtype='uint32'))

        f.close()
        first_cycle = False


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='Merge data generated by Jterator for use in tissueMAPS.')

    parser.add_argument('project_dir', nargs='*',
                        help='absolute path to project directory')

    parser.add_argument('-s', '--siteinfo', dest='siteinfo', required=True,
                        help='numpy matrix holding site info as generated by illuminati')

    parser.add_argument('-o', '--output', dest='output_dir', required=True,
                        help='directory where the HDF5 file should be saved')

    args = parser.parse_args()

    project_dir = args.project_dir[0]
    output_dir = args.output_dir
    siteinfo = np.load(args.siteinfo)

    if not project_dir:
        raise Exception('Project directory "%s" does not exist.' % project_dir)

    if not output_dir:
        raise Exception('Output directory "%s" does not exist.' % output_dir)

    merge_data(project_dir, output_dir, siteinfo)
