#! /usr/bin/env python
# encoding: utf-8
import os
import numpy as np
import pandas as pd
import h5py
import glob
import re
import tmlib
import dafu
import cluster
import utils
from tmlib.image import Image
from tmlib.experiment import Experiment


header_mapper = dafu.config['CHANNEL_MAPPER']

names = list()
count = 0
for i, cycle in header_mapper.iteritems():
    if isinstance(cycle, dict):
        for old, new in cycle.iteritems():
            count += 1
            names.append(new)

if len(np.unique(names)) < count:
    raise ValueError('Names have to be unique.')


def rename_features(feature_names, mapper):
    '''
    Rename features, i.e. replace substring in a feature name
    as defined in "mapper".

    Parameters
    ----------
    feature_names: List[str]
    mapper: Dict[str, str]
        key-value pairs -> substring to replace: new substring

    Returns
    -------
    List[str]
        renamed features
    '''
    for i, feature in enumerate(feature_names):
        for j, substring in enumerate(mapper.keys()):
            r = re.compile(substring)
            match = re.search(r, feature)
            if match:
                feature_names[i] = re.sub(mapper.keys()[j],
                                          mapper.values()[j],
                                          feature_names[i])
    return feature_names


def fuse_datasets(data_files, names, joblist):
    '''
    Fuse Jterator data of one project stored across several HDF5 files.

    Parameters
    ----------
    data_files: List[str]
        paths to Jterator data files
    names: List[str]
        names of the datasets in the HDF5 files
    joblist: List[Dict[str, str]]
        names of the image files that were used for the analysis

    Returns
    -------
    pandas.core.frame.DataFrame
        fused dataset with dimensions nxp,
        where n is the number of objects and p the number of features
    '''
    features = pd.DataFrame()
    for job_ix, filename in enumerate(data_files):

        f = h5py.File(filename, 'r')
        if not f.keys():
            raise Exception('File "%s" is empty' % filename)

        feature = pd.DataFrame()
        for feat_ix, name in enumerate(names):

            if name not in f.keys():
                raise ValueError('Dataset "%s" does not exist in file "%s'
                                 % (name, filename))

            ids = re.search(r'^([^_])_[^_]*ObjectId', name, re.IGNORECASE)
            if ids:
                obj_name = ids.group(1)
                # Get positional information from filename
                im_file = utils.image_name_from_joblist(joblist, filename)
                image = Image(im_file, tmlib.config)
                (row, column) = image.coordinates  # returns 0-based indices
                site = image.site
                # Store information about the site-specific information
                nitems = len(f.values()[0])
                feature['%s_ID_site' % obj_name] = np.repeat(site, nitems)
                feature['%s_ID_row' % obj_name] = np.repeat(row, nitems)
                feature['%s_ID_column' % obj_name] = np.repeat(column, nitems)
                feature['%s_ID_object' % obj_name] = f[ids.group(0)][:nitems]
            else:
                feat = f[name][()]
                if len(feat.shape) > 1:
                    raise ValueError('Only one dimensional feature vectors '
                                     'are supported.')
                feature[name] = feat

        f.close()

        if job_ix == 0:
            features = feature
        else:
            features = pd.concat([features, feature])

    return features


if __name__ == '__main__':
    import argparse

    print dafu.logo % {'version': dafu.__version__}

    descr = '''
    Fuse data generated by Jterator for use in TissueMAPS.
    Jterator generates an HDF5 file for each job. Load all datasets contained
    in the files of a particular experiment (which may be distributed across
    several subexperiments and/or multiple Jterator projects) and fuse and
    store them in a single `data.h5` HDF5 file.
    '''

    parser = argparse.ArgumentParser(description=descr)

    parser.add_argument('experiment_dir', nargs='*',
                        help='absolute path to experiment directory')

    parser.add_argument('--dafu_config', dest='dafu_config',
                        help='use custom yaml configuration file \
                             (defaults to "dafu" configuration)')

    parser.add_argument('-c', '--config', dest='config',
                        help='use custom yaml configuration file \
                             (defaults to "tmlib" configuration)')

    args = parser.parse_args()

    if args.config:
        # Overwrite default "tmlib" configuration
        print '. Using configuration file "%s"' % args.config
        args.config = tmlib.utils.load_config(args.config)
        tmlib.utils.check_config(args.config)
    else:
        args.config = tmlib.config

    if 'dafu_config' in args:
        if args.dafu_config:
            # Overwrite default "dafu" configuration
            print '. Using configuration file "%s"' % args.dafu_config
            args.config_file = args.config.copy()
            dafu_configuration = tmlib.utils.load_config(args.dafu_config)
            print '. Checking configuration file'
            # tmlib.dafu.utils.check_dafu_config(dafu_configuration)
        else:
            dafu_configuration = dafu.config
            args.config_file = ''
    else:
        dafu_configuration = dafu.config
        args.config_file = ''

    # Add "dafu" to "tmlib" configuration
    args.config.update(dafu_configuration)

    exp_dir = args.experiment_dir[0]

    if not exp_dir:
        raise ValueError('Project directory "%s" does not exist.' % exp_dir)

    experiment = Experiment(exp_dir, args.config)
    cycles = experiment.subexperiments
    output_filename = experiment.data_filename

    # Determine the Jterator project that was used to segment objects.
    # There should be only one and it will be treated separately.
    segmentation_project_dir = args.config['SEGMENTATION_PROJECT'].format(
                                        experiment_dir=exp_dir,
                                        experiment=os.path.basename(exp_dir),
                                        sep=os.path.sep)

    ###########################################################################
    #                 load individual datasets and fuse them                  #
    ###########################################################################

    # TODO: Are empty sites handles consistently?

    data_header = np.array(list(), dtype=np.string_)
    for i, c in enumerate(cycles):

        print('. Extracting features of cycle #%d: %s' %
              (c.cycle, c.name))

        cycle_dir = os.path.join(exp_dir, c.name)
        project_names = utils.list_jtprojects(cycle_dir)

        for project_name in project_names:

            project_dir = os.path.join(cycle_dir, project_name)

            print '.. Merging data of Jterator project "%s"' % project_name

            data_files = utils.list_data_files(project_dir)

            f = h5py.File(data_files[0], 'r')
            feature_names = f.keys()
            f.close()

            # We need retrieve the image filename from the joblist,
            # because we may need to obtain positional information from the
            # filename of the image, which was used to generate the data
            joblist_files = glob.glob(os.path.join(
                                os.path.dirname(data_files[0]), '..', '*jobs'))
            joblist = cluster.read_joblist(joblist_files[0])

            if not any([re.search(r'_OriginalObjectId$', name)
                        for name in feature_names]):
                raise ValueError('Files must contain a dataset containing '
                                 '"OriginalObjectId"')

            if project_dir == segmentation_project_dir:
                segm_ix = i
                segmentation_data = fuse_datasets(data_files,
                                                  names=feature_names,
                                                  joblist=joblist)
                continue

            features = fuse_datasets(data_files,
                                     names=feature_names,
                                     joblist=joblist)

            if i == 0:
                data = features
            else:
                data = pd.concat([data, features], axis=1)

    print '. Combining data from different cycles'
    data = data.T.groupby(level=0).first().T  # remove duplicate columns

    print '. Separate features belonging to different objects'
    objects = [re.match(r'^([^_]+)', name).group(1) for name in data.columns]
    (objects, object_ix) = np.unique(objects, return_inverse=True)

    ###########################################################################
    #                          write fused dataset                            #
    ###########################################################################

    print '. Writing fused data into HDF5 file "%s"' % output_filename
    f = h5py.File(output_filename, 'w')  # truncate file if exists
    for i, obj in enumerate(objects):

        print '. Writing data for objects "%s"' % obj

        obj_name = obj.lower()  # use lower case consistently

        # Store the actual features
        obj_data = data.filter(regex='^%s_(?!ID_)' % obj)
        location = 'objects/%s/features' % obj_name
        f.create_dataset(location, data=obj_data)
        f[location].attrs.__setitem__('names',
                                      np.array(obj_data.columns.values,
                                               dtype=np.string_))

        id_pattern = '^%s_ID_' % obj
        ids = data.filter(regex=id_pattern)

        # Build global object ids from local site-specific ids
        obj_global_ids = utils.build_global_ids(ids)

        # Store global object ids
        location = 'objects/%s/ids' % obj_name
        f.create_dataset(location, data=obj_global_ids.applymap(int))

        # Also store local "original" object ids for use outside of TissueMAPS
        location = 'objects/%s/original-ids' % obj_name
        f.create_dataset(location, data=ids.applymap(int))
        f[location].attrs.__setitem__('names',
                                      np.array(ids.columns.values,
                                               dtype=np.string_))

        # Determine parent-child relationship between objects
        try:
            parent_ids = data.filter(regex='^%s_ParentId' % obj)
            location = 'objects/%s/original-parent-ids' % obj_name
            f.create_dataset(location, data=parent_ids.applymap(int))
            # TODO: map to global ids
        except ValueError as e:
            # If the current object has no parent ids, it is the parent
            f.create_dataset('parent', data=np.string_(obj_name))

        # Calculate global centroids
        # For this we also need the dimensions of the images.
        # Since all images have the same size,
        # it's sufficient to obtain the dimensions of a single image.
        centroids = data.filter(regex='^%s_Centroid' % obj)
        # TODO: for now we assume that the last cycle is the reference cycle
        # that contains the segmentation images
        image_size = cycles[-1].project.segmentation_files[0].dimensions
        global_centroids = utils.calc_global_centroids(centroids, ids,
                                                       image_size)

        # Store global centroids
        location = 'objects/%s/centroids' % obj_name
        f.create_dataset(location, data=global_centroids.applymap(int))
        f[location].attrs.__setitem__('names',
                                      np.array(global_centroids.columns.values,
                                               dtype=np.string_))

        # Store index of objects at the border of images
        border_pattern = '^%s_BorderIx' % obj
        location = 'objects/%s/border' % obj_name
        border = data.filter(regex=border_pattern)
        if not border:
            border = data.filter(regex=border_pattern)
        f.create_dataset(location, data=border.applymap(int))

    f.close()
