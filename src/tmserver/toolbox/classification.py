import logging

from tmserver.toolbox.base import Classifier

logger = logging.getLogger(__name__)


class Classification(Classifier):

    __icon__ = 'SVC'

    __description__ = '''
        Classifies mapobjects based on the values of selected features and
        labels provided by the user.
    '''

    __methods__ = ['svm', 'rf']

    def label_feature_data_spark(self, feature_data, labeled_mapobjects):
        '''Adds labels to `feature_data` for supervised classification using
        the `Spark mllib <http://spark.apache.org/docs/latest/ml-guide.html>`_
        machine learning library.

        Parameters
        ----------
        feature_data: pyspark.sql.DataFrame
            data frame with "features" and "mapobject_id" columns as generated
            by :py:method:`tmserver.toolbox.classifier.Classfier.format_feature_data_spark`
        labeled_mapobjects: Tuple[int]
            ID and assigned label for each selected
            :py:class:`tmlib.models.Mapobject`

        Returns
        -------
        pyspark.sql.DataFrame
            subset of `feature_data` for selected mapobjects with additional
            "label" column
        '''
        labels = spark.sqlc.createDataFrame(
            labeled_mapobjects, schema=['mapobject_id', 'label']
        )
        labeled_data = feature_data.join(
            labels, labels['mapobject_id'] == feature_data['mapobject_id']
        ).cache()
        return labeled_data

    def label_feature_data_sklearn(self, feature_data, labeled_mapobjects):
        '''Adds labels to `feature_data` for supervised classification using
        the `scikit-learn <http://scikit-learn.org>`_ machine learning library.

        Parameters
        ----------
        feature_data: pandas.DataFrame
            data frame where columns are features and rows are mapobjects
            as generated by
            :py:method:`tmserver.toolbox.classifier.Classfier.format_feature_data_sklearn`
        labeled_mapobjects: Tuple[int]
            ID and assigned label for each selected
            :py:class:`tmlib.models.Mapobject`

        Returns
        -------
        pandas.DataFrame
            subset of `feature_data` for selected mapobjects with additional
            "label" column
        '''
        labeled_mapobjects = dict(labeled_mapobjects)
        ids = labeled_mapobjects.keys()
        labels = labeled_mapobjects.values()
        labeled_feature_data = feature_data[feature_data.index.isin(ids)].copy()
        labeled_feature_data['label'] = labels
        return labeled_feature_data

    def classify_sklearn(self, unlabeled_feature_data, labeled_feature_data,
            method):
        '''Trains a classifier for labeled mapobjects based on
        `labeled_feature_data` and predicts labels for all mapobjects in
        `unlabeled_feature_data`.

        Parameters
        ----------
        unlabeled_feature_data: pandas.DataFrame
        labeled_feature_data: pandas.DataFrame
        method: str
            model to use for classification

        Returns
        -------
        List[Tuple[int, str]]
            ID and predicted label for each mapobject
        '''
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.grid_search import GridSearchCV
        from sklearn import cross_validation, svm

        logger.info(
            'perform classification via Scikit-Learn with "%s" method', method
        )
        models = {
            'svm': svm.SVC,
            'rf': RandomForestClassifier
        }
        grid_search_space = {
            'svm': {
                'kernel': ['linear'],
                'C': np.linspace(0.1, 1, 5)
            },
            'rf': {
                'max_depth': [3, 5, 7],
                'min_samples_split': [1, 3, 10],
                'min_samples_leaf': [1, 3, 10]
            }
        }
        n_samples = labeled_feature_data.shape[0]
        n_folds = min(n_samples / 2, 10)

        X = labeled_feature_data.drop('label', axis=1)
        y = labeled_feature_data.label
        clf = models[method]()
        folds = cross_validation.StratifiedKFold(y, n_folds=n_folds)
        gs = GridSearchCV(clf, grid_search_space[method], cv=folds)
        logger.info('fit model')
        gs.fit(X, y)
        logger.info('collect predicted labels')
        predictions = gs.predict(unlabeled_feature_data)
        return zip(unlabeled_feature_data.index.tolist(), predictions.tolist())

    def _svm_spark(self, unlabeled_feature_data, label_df):
        from pyspark.mllib.classification import SVMWithSGD
        # TODO: grid search and crossvalidation
        label_rdd = label_df.\
            select('indexedLabel', 'features').\
            map(lambda row: LabeledPoint(row.indexedLabel, row.features))
        svm = SVMWithSGD.train(label_rdd, intercept=True)
        predictions = unlabeled_feature_data.\
            select('mapobject_id', 'features').\
            map(lambda row: (row.mapobject_id, label_mapping[svm.predict(row.features)]))
        return predictions.collect()

    def classify_spark(self, unlabeled_feature_data, labeled_feature_data):
        '''Trains a classifier for labeled mapobjects based on
        `labeled_feature_data` and predicts labels for all mapobjects in
        `unlabeled_feature_data`.

        Parameters
        ----------
        unlabeled_feature_data: pyspark.sql.DataFrame
        labeled_feature_data: pyspark.sql.DataFrame

        Returns
        -------
        List[Tuple[int, str]]
            ID and predicted label for each mapobject
        '''
        from pyspark.ml import Pipeline
        from pyspark.ml.feature import StringIndexer
        from pyspark.ml.feature import VectorAssembler
        from pyspark.ml.feature import VectorIndexer
        from pyspark.ml.feature import VectorAssembler, VectorIndexer, StringIndexer
        from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
        from pyspark.ml.classification import RandomForestClassifier
        from pyspark.ml.evaluation import MulticlassClassificationEvaluator

        logger.info('perform classification via Spark with "%s" method', method)
        feature_indexer = VectorIndexer(
                inputCol='features', outputCol='indexedFeatures',
                maxCategories=2
            ).\
            fit(labeled_feature_data)

        label_indexer = StringIndexer(
                inputCol='label', outputCol='indexedLabel'
            ).\
            fit(labeled_feature_data)

        label_df = label_indexer.transform(labeled_feature_data)
        label_mapping = {
            r.indexedLabel: r.label
            for r in label_df.select('label','indexedLabel').distinct().collect()
        }
        # TODO: How can this be achieved with IndexToString() when prediction
        # is done on unlabeled dataset?
        # label_converter = IndexToString(
        #     inputCol='prediction', outputCol='predictedLabel',
        #     labels=label_indexer.labels
        # )

        if method == 'svm':
            # SVM is not yet implemented in the Spark DataFrame interface, but
            # in the old RDD interface. Therefore, we have to implemnt this
            # method separately.
            return self._svm_spark(unlabeled_feature_data, label_df)
        else:
            models = {
                'rf': RandomForestClassifier
            }
            grid_search_space = {
                'rf': {
                    'maxDepth': [3, 5, 7],
                    'numTrees': [10, 20, 30]
                }
            }

            clf = models[method](
                labelCol='indexedLabel', featuresCol='indexedFeatures'
            )
            grid = ParamGridBuilder()
            for k, v in grid_search_space.iteritems():
                grid.addGrid(getattr(clf, k), v)
            grid.build()

            pipeline = Pipeline(stages=[feature_indexer, label_indexer, clf])
            evaluator = MulticlassClassificationEvaluator(
                labelCol='indexedLabel', predictionCol='prediction',
                metricName='f1'
            )
            crossval = CrossValidator(
                estimator=pipeline, estimatorParamMaps=grid,
                evaluator=evaluator, numFolds=3
            )
            logger.info('fit model')
            model = crossval.fit(labeled_feature_data)
            predictions = model.transform(unlabeled_feature_data)
            logger.info('collect predicted labels')
            result = predictions.select('mapobject_id', 'prediction').collect()
            return [
                (r.mapobject_id, label_mapping[r.prediction]) for r in result
            ]

    def process_request(self, payload, session_id, experiment_id, use_spark=False):
        # Get mapobject
        mapobject_type_name = payload['chosen_object_type']
        feature_names = payload['selected_features']
        method = payload['method']

        if method not in self.__methods__:
            raise ValueError('Unknown method "%s".' % method)

        with tm.utils.ExperimentSession(experiment_id) as session:
            mapobject_type_id = session.query(tm.MapobjectType.id).\
                filter_by(name=mapobject_type_name).\
                one()

        labeled_mapobjects = list()
        color_map = dict()
        for cls in payload['training_classes']:
            labels = [(i, cls['name']) for i in cls['object_ids']]
            labeled_mapobjects.extend(labels)
            color_map[cls['name']] = cls['color']

        if use_spark:
            from pyspark.sql import DataFrameReader
            from pyspark.ml.feature import VectorAssembler, VectorIndexer, StringIndexer
            from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
            from pyspark.ml.classification import RandomForestClassifier
            from pyspark.ml.evaluation import MulticlassClassificationEvaluator
            unlabeled_feature_data = self.format_feature_data_spark(
                experiment_id, mapobject_type_name, feature_names
            )
            labeled_feature_data = self.label_feature_data_spark(
                unlabeled_feature_data, labeled_mapobjects
            )
            predicted_labels = self.classify_spark(
                unlabeled_feature_data, labeled_feature_data, method
            )
        else:
            unlabeled_feature_data = self.format_feature_data_sklearn(
                experiment_id, mapobject_type_name, feature_names
            )
            labeled_feature_data = self.label_feature_data_sklearn(
                unlabeled_feature_data, labeled_mapobjects
            )
            predicted_labels = self.classify_sklearn(
                unlabeled_feature_data, labeled_feature_data, method
            )

        return ToolResult(
            session_id=session_id,
            layer=SupervisedClassifierLabelLayer(
                mapobject_type_id=mapobject_type_id,
                labels=dict(predicted_labels),
                color_map=color_map
            )
        )

