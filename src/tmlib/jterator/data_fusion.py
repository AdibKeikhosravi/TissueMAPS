import numpy as np
from ..readers import DatasetReader
from ..writers import DatasetWriter
from ..errors import DataError


def combine_datasets(data_files):
    '''
    Combine data stored across several HDF5 files into one HDF5 file.

    Parameters
    ----------
    data_files: List[str]
        paths to Jterator data files that were generated by individual jobs

    Returns
    -------
    Dict[str, pandas.core.frame.DataFrame or numpy.ndarray]
        fused datasets for each dataset; the key specifies the full path to the
        location of the dataset within the HDF5 file
    '''
    data = dict()
    with DatasetReader(data_files[0]) as f:
        object_names = f.list_groups('/objects')

    for i, filename in enumerate(data_files):

        with DatasetReader(filename) as f:

            # Collect metadata per site, i.e. per job ID
            metadata_path = '/metadata'

            if not f.exists(metadata_path):
                raise DataError(
                        'Data file must contain group "%s"' % metadata_path)

            job_ids = map(int, f.list_groups(metadata_path))
            for j in job_ids:
                j_group = '{group}/{subgroup}'.format(
                                group=metadata_path, subgroup=str(j))
                dataset_names = f.list_datasets(j_group)
                for name in dataset_names:
                    dataset_path = '{group}/{dataset}'.format(
                                        group=j_group, dataset=name)
                    data[dataset_path] = f.read(dataset_path)

            # Collect features and segmentations per object
            for obj_name in object_names:
                features_path = 'objects/%s/features' % obj_name
                segmentation_path = 'objects/%s/segmentation' % obj_name

                if f.exists(features_path):
                    dataset_names = f.list_datasets(features_path)
                    for name in dataset_names:
                        dataset_path = '{group}/{dataset}'.format(
                                        group=features_path, dataset=name)
                        if i == 0:
                            data[dataset_path] = f.read(dataset_path)
                        else:
                            data[dataset_path] = np.array(
                                np.append(
                                    data[dataset_path],
                                    f.read(dataset_path)
                                ),
                                dtype=data[dataset_path].dtype
                            )

                if f.exists(segmentation_path):
                    dataset_names = f.list_datasets(segmentation_path)
                    for name in dataset_names:
                        dataset_path = '{group}/{dataset}'.format(
                                        group=segmentation_path, dataset=name)
                        if i == 0:
                            data[dataset_path] = f.read(dataset_path)
                        else:
                            data[dataset_path] = np.array(
                                np.append(
                                    data[dataset_path],
                                    f.read(dataset_path)
                                ),
                                dtype=data[dataset_path].dtype
                            )
                    group_names = f.list_groups(segmentation_path)
                    for g_name in group_names:
                        group_path = '{group}/{subgroup}'.format(
                                        group=segmentation_path,
                                        subgroup=g_name)
                        dataset_names = f.list_datasets(group_path)
                        for name in dataset_names:
                            dataset_path = '{group}/{dataset}'.format(
                                            group=group_path, dataset=name)
                            if g_name == 'image_dimensions':
                                if i == 0:
                                    data[dataset_path] = f.read(dataset_path)
                            else:
                                if i == 0:
                                    data[dataset_path] = f.read(dataset_path)
                                else:
                                    data[dataset_path] = np.array(
                                        np.append(
                                            data[dataset_path],
                                            f.read(dataset_path)
                                        ),
                                        dtype=data[dataset_path].dtype
                                    )

    return data


def update_datasets(old_filename, new_filename):
    '''
    Recursively copy all datasets from one HDF5 file to a new HDF5 file without
    overwriting existing ones.

    Parameters
    ----------
    old_filename: str
        absolute path to the file whose content should be copied
    new_filename: str
        absolute path to the file, which should be updated
    '''
    with DatasetReader(old_filename) as old_file:
        with DatasetWriter(new_filename) as new_file:
            def copy_recursively(p):
                groups = old_file.list_groups(p)
                for g in groups:
                    group_path = '{group}/{subgroup}'.format(
                                    group=p, subgroup=g)
                    if g == 'coordinates':
                        # The coordinates for the visualization have to be
                        # recalculated in case the segmentation has changed.
                        continue
                    for d in old_file.list_datasets(group_path):
                        dataset_path = '{group}/{dataset}'.format(
                                            group=group_path, dataset=d)
                        if not new_file.exists(dataset_path):
                            # Keep the more recent dataset.
                            new_file.write(
                                    dataset_path,
                                    data=old_file.read(dataset_path))
                    copy_recursively(group_path)
            copy_recursively('/')
