import logging
from ..readers import Hdf5Reader
from ..writers import Hdf5Writer

logger = logging.getLogger(__name__)


def merge_datasets(input_files, output_file):
    '''
    Combine datasets stored across several HDF5 files into a single dataset
    in one HDF5 file.

    Parameters
    ----------
    input_files: List[str]
        paths to Jterator data files that were generated by individual jobs
    output_file: str
        path to the final data file that should contain the entire datasets
    '''
    with Hdf5Writer(output_file) as store:

        for i, filename in enumerate(input_files):
            logger.debug('process datasets of file %s')

            with Hdf5Reader(filename) as data:

                object_names = data.list_groups('/objects')
                if len(object_names) == 0:
                    logger.warn('no objects in file %s', filename)
                for obj in object_names:
                    obj_path = '/objects/%s' % obj
                    feat_path = '%s/features' % obj_path
                    if not data.exists(feat_path):
                        logger.warn('no features for "%s" objects in file %s',
                                    obj, filename)
                        continue
                    features = data.list_datasets(feat_path)
                    for feat in features:
                        f_path = '%s/%s' % (feat_path, feat)
                        # Remove border objects
                        feat_data = data.read(f_path)
                        if len(feat_data) > 0:
                            store.append(f_path, feat_data)
